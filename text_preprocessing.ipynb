{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wCJUE2xdyRjl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## REGEX "
      ],
      "metadata": {
        "id": "DcX1S28g2Mig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* RegEx uses metacharacters in conjunction with a search engine to retrieve specific patterns. \n",
        "\n",
        "* Metacharacters are the building blocks of regular expressions. \n",
        "\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Meta character</th>\n",
        "    <th>Meaning</th>\n",
        "    <th>Example/</th>\n",
        "  </tr>\n",
        "  <tr> \n",
        "    <td> \\d </td> \n",
        "    <td> Whole Number 0-9 </td> \n",
        "    <td> \\d\\d = 81, \\d=4 </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> [a-z], [0-9] </td> \n",
        "    <td> Character set, at least one of which must be a match, but no more than one unless otherwise specified. \n",
        "    The order of the characters does not matter. </td>\n",
        "    <td> [a-z] = a, b, c,... / [0-9] = 0, 1, 2... </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> * </td> \n",
        "    <td>  Asterisk matches when the character preceding \\* matches 0 or more times </td> \n",
        "    <td> tre*= tr / tree / treeeeee </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> + </td>\n",
        "    <td> Matches when the character preceding + matches 1 or more times </td>\n",
        "    <td> tre*= tre / tree / treeeeee </td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> {n} </td> \n",
        "    <td> Fixes number of occurences </td>\n",
        "    <td> \\d{3} = 123/ 234/ 987. \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> . (period) </td> \n",
        "    <td> Matches any single alphanumeric character or symbol </td> \n",
        "    <td> ton. = tone / ton3 / tone# </td> \n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "* What will be the regex for your phone number?\n",
        "* What will \".*\" match?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "You can use regular expressions to search for URLs, email addresses, dates, and other strings that follow a specific pattern. You need to understand the pattern well!!!\n",
        "\n"
      ],
      "metadata": {
        "id": "126SGofmRN03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "m-nioK_hPMXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Search**"
      ],
      "metadata": {
        "id": "qe_jfSH8v59U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"The rain in Spain\"\n",
        "x = re.search(\".*a\\.n.*\", txt)    #returns true if there is a match anywhere in the string\n",
        "\n",
        "if x:\n",
        "  print(\"YES! We have a match!\")\n",
        "else:\n",
        "  print(\"No match\")"
      ],
      "metadata": {
        "id": "ATrcqlbMWG8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d728ad19-bef2-4d00-9f41-f4ad1708924d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No match\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Find**"
      ],
      "metadata": {
        "id": "lvWPrwOYv-lU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = re.findall(\"[^\\sin ]\", txt)     #returns a list of ocurences contatining the word\n",
        "print(x)"
      ],
      "metadata": {
        "id": "RlNTw_iIWX6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79e80600-e90a-4111-b406-738b8ae6ff86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T', 'h', 'e', 'r', 'a', 'S', 'p', 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split**"
      ],
      "metadata": {
        "id": "hFNeNUSRxBv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = re.split(\"\\s\", txt)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "uBxxjS1bWbGt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "815d1134-4589-40ad-a1e2-5f478195e8e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'rain', 'in', 'Spain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt2 = \"My name is, megha?? I love trees!!\"\n",
        "x = re.split(\"[\\?\\!]\", txt2)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "vlX2McuaWdK7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee047fca-0b6a-4eb8-943c-a5b43fdc1e25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['My name is, megha', '', ' I love trees', '', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Replace**"
      ],
      "metadata": {
        "id": "aBrrgiKkxrRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = re.sub(\"\\s\", \"a+\", txt)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "vZmOtzm0WkEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14c10122-ae8a-4c9b-8ea3-4293083bd87f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thea+raina+ina+Spain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# controlled replace\n",
        "x = re.sub(\"\\s\", \"+\", txt, 2)   #second terms cant be regex\n",
        "print(x) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c-r4viBx41D",
        "outputId": "ff6ef0c9-9619-4584-eda1-576adb2eb954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The+rain+in Spain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qjAwc-CtPFVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rf_1BYudPGgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d311RhmjPGce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_NYG45rVPI-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xipxBdOTPPBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK"
      ],
      "metadata": {
        "id": "wCJUE2xdyRjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDmG7QsEyb2w",
        "outputId": "6543c532-835d-4665-e628-c9d84a982bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer('\\w+')\n",
        "text = \"Hello, she has build a very good site. She hasn't added any irrelevant info!\"\n",
        "filterdText = tokenizer.tokenize(text)\n",
        "print(filterdText)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvQfJ82G2K4l",
        "outputId": "db3ad488-69a4-4e75-9429-e4a7b6e70347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'she', 'has', 'build', 'a', 'very', 'good', 'site', 'She', 'hasn', 't', 'added', 'any', 'irrelevant', 'info']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "# \\w+|[^\\w\\s]+\n",
        "tokenizer = WordPunctTokenizer()\n",
        "text = \"Hello, she has build a very good site. She hasn't added any irrelevant info!\"\n",
        "filterdText = tokenizer.tokenize(text)\n",
        "print(filterdText)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXg4b5BbHPhC",
        "outputId": "2fe3d09f-d7ee-4366-d582-10ede3da37fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'she', 'has', 'build', 'a', 'very', 'good', 'site', '.', 'She', 'hasn', \"'\", 't', 'added', 'any', 'irrelevant', 'info', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Hello, she has build a very good site. She hasn't added any irrelevant info!\"\n",
        "filterdText = word_tokenize(text)\n",
        "print(filterdText)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDJe4dGP6JRD",
        "outputId": "86c934bb-15b3-45a2-ffea-20da423fdc8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'she', 'has', 'build', 'a', 'very', 'good', 'site', '.', 'She', 'has', \"n't\", 'added', 'any', 'irrelevant', 'info', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence Tokenization"
      ],
      "metadata": {
        "id": "FKYFnfLY6lXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"Dr. Hello NLTK, You have build a very good site. I love visiting your site!\"\n",
        "filterdText = sent_tokenize(text)\n",
        "print(filterdText)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQAdX1sq6pXe",
        "outputId": "614b675c-a852-49d8-803d-38421af6fedb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Dr. Hello NLTK, You have build a very good site.', 'I love visiting your site!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing StopWords"
      ],
      "metadata": {
        "id": "oRoetCYc8PUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzUmUTDP6x4E",
        "outputId": "3c7a9fc1-a2f6-45a3-bff5-a740f4652be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)\n",
        "print(len(stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqdPSkgN8Vh8",
        "outputId": "44749e86-5682-42bc-951a-6df18922d579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"should've\", 'yourself', 'why', 'hers', 'until', 's', 'our', 'how', 'mightn', 'all', 'my', \"wouldn't\", 'on', 'any', 'while', 'after', 'herself', 'nor', 'by', \"mustn't\", 'to', 'at', 'i', 'this', 'be', 'does', 'them', 'where', 'between', 'doesn', 'not', 'the', \"shan't\", 'did', 'in', 'ourselves', 'their', 'than', 'him', 'further', 'ain', 'haven', \"isn't\", 'from', 'whom', 'which', 'is', 'having', 'with', 'hadn', 'these', 'a', 'but', 'won', \"hadn't\", 'just', 'been', 'when', 'wouldn', 'before', 'against', 'more', 'don', \"that'll\", 'had', 'ma', \"couldn't\", \"didn't\", 'o', 't', 'doing', 'through', 'isn', 'was', 'ours', 'we', 'no', 'hasn', 'what', 'will', 'me', 'out', 'has', 'there', 'about', 'under', 'if', 'very', \"you'll\", 'above', 'now', 'yourselves', 'am', 'you', 'are', 'll', 'were', 'over', 'that', 'because', 'up', 'both', 've', 'he', 'or', 'she', 'into', 'off', 'd', 'weren', 'each', 'too', 'shouldn', 'wasn', 'have', 'here', 'few', 'it', 'and', 'couldn', 'for', 'itself', 'again', 'of', 'its', \"wasn't\", 'theirs', 'some', 'they', 'themselves', 'same', 'her', 'didn', 'so', \"you've\", \"don't\", 'during', 'who', \"needn't\", \"she's\", 'do', \"haven't\", \"weren't\", \"hasn't\", \"shouldn't\", \"you'd\", 'm', 'your', 'as', 'should', 'shan', 're', 'then', 'his', 'once', 'yours', \"you're\", \"it's\", 'mustn', 'y', 'other', \"mightn't\", 'needn', \"won't\", 'own', \"aren't\", 'an', 'can', 'such', 'only', 'himself', 'below', 'down', 'most', \"doesn't\", 'being', 'aren', 'those', 'myself'}\n",
            "179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "tokens = word_tokenize(input_str)\n",
        "result = [i for i in tokens if i not in stop_words]\n",
        "print (result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idXeDx0W8bAG",
        "outputId": "eeaca815-aaf4-48ff-dc1c-d3e2341e78e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "Stemming is a process of linguistic normalization, which reduces words to their word root word or chops off the derivational affixes. For example, connection, connected, connecting word reduce to a common word \"connect\"."
      ],
      "metadata": {
        "id": "3ZY2L6dP8ep_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Gk4_IKjTUPFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "input_str =\"been had done languages cities mice fairly\"\n",
        "input_str = word_tokenize(input_str)\n",
        "for word in input_str:\n",
        "    print(stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAiW6Qzh8gxG",
        "outputId": "52b92e00-3e30-418d-e5d6-4799a08f93db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "been\n",
            "had\n",
            "done\n",
            "languag\n",
            "citi\n",
            "mice\n",
            "fairli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "lancaster = LancasterStemmer()\n",
        "input_str =\"been had done languages cities mice fairly\"\n",
        "input_str = word_tokenize(input_str)\n",
        "for word in input_str:\n",
        "    print(lancaster.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gXZORXMETo_",
        "outputId": "eb715840-409f-4bb2-bd8d-65e951a598d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "been\n",
            "had\n",
            "don\n",
            "langu\n",
            "city\n",
            "mic\n",
            "fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer # This is \"Porter 2\" and is considered the optimal stemmer.\n",
        "\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "input_str =\"been had done languages cities mice fairly\"\n",
        "input_str = word_tokenize(input_str)\n",
        "for word in input_str:\n",
        "    print(snowball.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDlLAslQEgkj",
        "outputId": "808a071b-3c1d-4e25-b7e1-4fd7df70bcba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "been\n",
            "had\n",
            "done\n",
            "languag\n",
            "citi\n",
            "mice\n",
            "fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization\n",
        "Lemmatization reduces words to their base word, which is linguistically correct lemmas. It transforms root word with the use of vocabulary and morphological analysis. "
      ],
      "metadata": {
        "id": "PXFo05KI8p-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7tmMVvC8oV6",
        "outputId": "56c26e91-e561-490d-ba69-3404744e0556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "input_str = \"been had done languages cities mice fairly\"\n",
        "input_str = word_tokenize(input_str)\n",
        "for word in input_str:\n",
        "    print(lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gJ7wcGq8wJg",
        "outputId": "24762c7f-4b36-431b-a04a-1e961b8dc2cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "been\n",
            "had\n",
            "done\n",
            "language\n",
            "city\n",
            "mouse\n",
            "fairly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming VS Lemmatization\n",
        "Lemmatization is usually more sophisticated than stemming. Stemmer works on an individual word without knowledge of the context. For example, The word \"better\" has \"good\" as its lemma. This thing will miss by stemming because it requires a dictionary look-up."
      ],
      "metadata": {
        "id": "OUMVO3y8KiD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stem = PorterStemmer()\n",
        "\n",
        "word = \"better\"\n",
        "\n",
        "print(\"Lemmatized Word:\", lem.lemmatize(word, wordnet.ADJ)) \n",
        "print(\"Stemmed Word:\", stem.stem(word))\n",
        "\n",
        "print()\n",
        "\n",
        "word = \"flying\"\n",
        "print(\"Lemmatized Word:\", lem.lemmatize(word, wordnet.VERB)) \n",
        "print(\"Stemmed Word:\", stem.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45fvoEWZKhYT",
        "outputId": "fb4a4ef9-efcf-48a4-d55d-1f60118d39a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Word: good\n",
            "Stemmed Word: better\n",
            "\n",
            "Lemmatized Word: fly\n",
            "Stemmed Word: fli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-Of-Speech Tagging"
      ],
      "metadata": {
        "id": "IBymHjMP9E-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxMNG21y9Hj5",
        "outputId": "68a620a2-55c5-49dd-9e82-8d3256db90a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello NLTK, You have build a very good site. I love visiting your site!\"\n",
        "sentence = nltk.sent_tokenize(text)\n",
        "for sent in sentence:\n",
        "    print(nltk.pos_tag(nltk.word_tokenize(sent)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncBg3St19OCP",
        "outputId": "f0e04daa-0611-44e9-87c4-ce59157136a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Hello', 'NNP'), ('NLTK', 'NNP'), (',', ','), ('You', 'PRP'), ('have', 'VBP'), ('build', 'VBN'), ('a', 'DT'), ('very', 'RB'), ('good', 'JJ'), ('site', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('love', 'VBP'), ('visiting', 'VBG'), ('your', 'PRP$'), ('site', 'NN'), ('!', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization with POS Tagging"
      ],
      "metadata": {
        "id": "KPYnfA1m9WeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "3XjM2-B-9Uzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_tagger(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:         \n",
        "        return None"
      ],
      "metadata": {
        "id": "x5eAD34g9efq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"been had done languages cities mice fairly\"\n",
        "print(\"{0:20}{1:20}\".format(\"Without POS\",\"With POS\"))\n",
        "sentence = nltk.sent_tokenize(text)\n",
        "for sent in sentence:\n",
        "    tok = nltk.pos_tag(nltk.word_tokenize(sent))\n",
        "    for word in tok:\n",
        "      pos = pos_tagger(word[1])\n",
        "      if pos:\n",
        "        print(\"{0:20}{1:20}\".format(lemmatizer.lemmatize(word[0]), lemmatizer.lemmatize(word[0], pos)))\n",
        "      else:\n",
        "        print(\"{0:20}{1:20}\".format(lemmatizer.lemmatize(word[0]), lemmatizer.lemmatize(word[0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfMR7-kT9izM",
        "outputId": "6a806b51-84b1-4439-831b-a1e725907993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without POS         With POS            \n",
            "been                be                  \n",
            "had                 have                \n",
            "done                do                  \n",
            "language            language            \n",
            "city                city                \n",
            "mouse               mice                \n",
            "fairly              fairly              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synsets in Wordnet\n",
        "Synset are groupings of synonyms words that express the same concept. When you use Wordnet to look up words, you will get a list of Synset instances. "
      ],
      "metadata": {
        "id": "NFjMC_UxO8F_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "syn = wn.synsets('dog')[0]"
      ],
      "metadata": {
        "id": "Zgns9WxYPGEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References. \n",
        "\n",
        "1. [Text Processing Tutorial](https://colab.research.google.com/drive/1C8K5yqdjI0LJ1CMvNc-uxwTuMCHFjYMY#scrollTo=7xbKjI6fM_cX)\n",
        "2. [Hands-On nltk tutorial](https://github.com/hb20007/hands-on-nltk-tutorial)\n",
        "3. [Natural Language Toolkit - Tokenizing Text](https://www.tutorialspoint.com/natural_language_toolkit/natural_language_toolkit_tokenizing_text.htm)\n",
        "4. [Text Analytics - nltk](https://www.datacamp.com/tutorial/text-analytics-beginners-nltk)\n",
        "5. [Stemming VS Lemmatization](https://towardsdatascience.com/stemming-vs-lemmatization-in-nlp-dea008600a0)\n",
        "6. [NLTK Documentation](https://www.nltk.org/howto.html)\n"
      ],
      "metadata": {
        "id": "dSuUNYS69m88"
      }
    }
  ]
}